{"cells":[{"cell_type":"markdown","source":["## National Stock Exchange of India for all series. \n\nA brief description of columns:\n* SYMBOL: Symbol of the listed company. \n* SERIES: Series of the equity. Values are [EQ, BE, BL, BT, GC and IL] \n* OPEN: The opening market price of the equity symbol on the date. \n* HIGH: The highest market price of the equity symbol on the date. \n* LOW: The lowest recorded market price of the equity symbol on the date. \n* CLOSE: The closing recorded price of the equity symbol on the date. \n* LAST: The last traded price of the equity symbol on the date. \n* PREVCLOSE: The previous day closing price of the equity symbol on the date. \n* TOTTRDQTY: Total traded quantity of the equity symbol on the date. \n* TOTTRDVAL: Total traded volume of the equity symbol on the date. \n* TIMESTAMP: Date of record. \n* TOTALTRADES: Total trades executed on the day. \n* ISIN: International Securities Identification Number."],"metadata":{}},{"cell_type":"code","source":["spark"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["%fs ls /FileStore/tables/"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%fs head /FileStore/tables/FINAL_FROM_DF.csv"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["from pyspark.sql.types import *\ninputPath=\"/FileStore/tables/\"\n# Since we know the data format already, let's define the schema to speed up processing (no need for Spark to infer schema)\ncsvSchema = StructType([ StructField(\"SYMBOL\", StringType(), True), StructField(\"SERIES\", StringType(), True),StructField(\"OPEN\", FloatType(), True),StructField(\"HIGH\", FloatType(), True),StructField(\"LOW\", FloatType(), True),StructField(\"CLOSE\", FloatType(), True),StructField(\"LAST\", FloatType(), True),StructField(\"PREVCLOSE\", FloatType(), True),StructField(\"TOTTRDQTY\", FloatType(), True),StructField(\"TOTTRDVAL\", FloatType(), True),StructField(\"TIMESTAMP\", TimestampType(), True),StructField(\"TOTALTRADES\", FloatType(), True),StructField(\"ISIN\", StringType(), True)])\n\n# Static DataFrame representing data in the CSV file\nstaticInputDF = (\n  spark\n    .read\n    .schema(csvSchema)\n    .csv(inputPath)\n)\n\ndisplay(staticInputDF)"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["from pyspark.sql.functions import *      # for window() function\n\nstaticCountsDF = (\n  staticInputDF\n    .groupBy(\n       staticInputDF.SERIES, \n       window(staticInputDF.TIMESTAMP, \"1 day\"))    \n    .count()\n)\nstaticCountsDF.cache()\n\n# Register the DataFrame as table 'static_counts'\nstaticCountsDF.createOrReplaceTempView(\"static_counts\")"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%sql select SERIES,sum(count) as total_count from static_counts where SERIES IN ('EQ','NC','BZ') group by SERIES"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%sql select SERIES, date_format(window.end, \"MMM-dd HH:mm\") as time, count from static_counts \nwhere SERIES IN ('EQ','NC','BZ') order by 'TIMESTAMP', SERIES"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\n# Similar to definition of staticInputDF above, just using `readStream` instead of `read`\nstreamingInputDF = (\n  spark\n    .readStream                       \n    .schema(csvSchema)               # Set the schema of the JSON data\n    .option(\"maxFilesPerTrigger\", 1)  # Treat a sequence of files as a stream by picking one file at a time\n    .csv(inputPath)\n)\n\n# Same query as staticInputDF\nstreamingCountsDF = (                 \n  streamingInputDF\n    .groupBy(\n      streamingInputDF.SERIES, \n      window(streamingInputDF.TIMESTAMP, \"1 day\"))\n    .count()\n)\n\n# Is this DF actually a streaming DF?\nstreamingCountsDF.isStreaming"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n\nquery = (\n  streamingCountsDF\n    .writeStream\n    .format(\"memory\")        # memory = store in-memory table (for testing only in Spark 2.0)\n    .queryName(\"counts\")     # counts = name of the in-memory table\n    .outputMode(\"complete\")  # complete = all the counts should be in the table\n    .start()\n)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["from time import sleep\nsleep(5)  # wait a bit for computation to start"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%sql select SERIES, date_format(window.end, \"MMM-dd HH:mm\") as time, count from static_counts \nwhere SERIES IN ('EQ','NC','BZ') order by 'TIMESTAMP', SERIES"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["sleep(30)  # wait a bit more for more data to be computed"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["%sql select SERIES, date_format(window.end, \"MMM-dd HH:mm\") as time, count from static_counts \nwhere SERIES IN ('EQ','NC','BZ') order by 'TIMESTAMP', SERIES"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["%sql select SERIES,sum(count) as total_count from static_counts where SERIES IN ('EQ','NC','BZ') group by SERIES"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":16}],"metadata":{"name":"Streaming NSE Stocks dataset","notebookId":264106917855261},"nbformat":4,"nbformat_minor":0}
